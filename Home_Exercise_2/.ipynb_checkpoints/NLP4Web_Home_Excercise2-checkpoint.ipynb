{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d28de550dffdd78ce6594542550d653",
     "grade": false,
     "grade_id": "cell-4d87ae78fb4caf9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# NLP and the Web: Home Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1145e2ba69b3e56620106550f0984e0",
     "grade": false,
     "grade_id": "cell-8b531762670192f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task 0 ~ 0P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d4cef5c9a404a5516183e6ce9bf2128",
     "grade": false,
     "grade_id": "cell-6eca7b6c6c224dde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### a) Please enter your group number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75470a4986487fdb40d73fa368335c95",
     "grade": true,
     "grade_id": "cell-cccfbf605a28a18a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1865c1dc9e1bcc5be8b5d3354eafe868",
     "grade": false,
     "grade_id": "cell-2ed8ca1b9e5204c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_**Regarding types, documentation, and output:**_\n",
    "\n",
    "_We tried to make the description of the parameters as clear as possible. However, if you believe that something is missing, please reach out to us in Moodle. We provide type hints for the function parameters and return values of the functions that you have to implement. These are suggestions only, and you may use different types if you prefer._\n",
    "\n",
    "_Nevertheless, your code must use the provided method stubs and parameters. Furthermore, make sure that your code runs without errors and in a reasonable amount of time, for example by using \"Kernel/Restart & Run All\" before submitting._\n",
    "\n",
    "_Please use comments where appropriate to help the tutors understand your code. This is especially important for the more extensive exercises later on. Finally, please pay attention to how you output the results. We highly recommend using `display(df)` for displaying data frames._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0feae69340bf259c5e31bf2daa914a3",
     "grade": false,
     "grade_id": "cell-e69442a7dc08d8fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "479253f9985e09df9c960da6db88bdc5",
     "grade": false,
     "grade_id": "cell-5e9358cbfb48b313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8132cf9df7897906460cecd5505615a5",
     "grade": false,
     "grade_id": "cell-4fb173363b87fa49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Neural Network from scratch ~ 10P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c50f13a89265063585d6ddfa44683278",
     "grade": false,
     "grade_id": "cell-8b9e11b86587a53b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**a) What is a Perceptron and what are the tasks of the 5 different components? (1P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fe0bfe53f31cfbc53ab8b10ee37b631",
     "grade": true,
     "grade_id": "cell-b61af27cdc9fc323",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c40b9fb7be1ced0bcd150796cea9ecc9",
     "grade": false,
     "grade_id": "cell-00fea53a9accf126",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b) Which activation functions do you know besides the ReLU (name at least 3) and explain the inputs / outputs of one of those? (1.5P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b92eca56f2db753e55146acb64d95937",
     "grade": true,
     "grade_id": "cell-0afb53fc3ad6f343",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b2df6568ebe7976ce5a8256a3e21e09",
     "grade": false,
     "grade_id": "cell-474539096c395ac9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**c) Implement the ReLU activation function and its derivative. (2P)**\n",
    "\n",
    "*Note:* The ReLU (Rectified Linear Unit) function is a piecewise linear function which is very similar to the linear activation function that you know from lecture 2 page 30. The difference between these two is that ReLU cuts of the negative value and sets them to zero. $$\\text{ReLU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ 0, & \\text{if } x \\le 0 \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bdca5bde4a54c4f12682e21828fd0e3",
     "grade": true,
     "grade_id": "cell-59f29cac5e1ab450",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def relu(z: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    '''\n",
    "    Input: \n",
    "        z: floating point vector to which you want to apply the ReLU activation function\n",
    "\n",
    "    Output:\n",
    "        Output a floating point vector that has the same shape as z but with the activated values\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "relu(np.array([1,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e9ffea2f91033d0b1f2fdac3dd56de",
     "grade": true,
     "grade_id": "cell-7cc5e0bafd23eda9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are some test cases for you to check your implementation\n",
    "assert((relu(np.array([1,-1])) == np.array([1,0])).all())\n",
    "assert((relu(np.array([-112,234])) == np.array([0,234])).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54aa31addfec1342e366079c9c23807e",
     "grade": true,
     "grade_id": "cell-25722a6942cadbc3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_relu(z: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    '''\n",
    "    Input: \n",
    "        z: Vector to which you want to apply the gradient of the ReLU activation function\n",
    "\n",
    "    Output:\n",
    "        Output a vector that has the same shape as z and is the derivative of z\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4078ba101a72af211392857e6ccae8ca",
     "grade": true,
     "grade_id": "cell-a7a9a25aa59363f7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are some test cases for you to check your implementation\n",
    "assert((grad_relu(np.array([1,-1])) == np.array([1,0])).all())\n",
    "assert((grad_relu(np.array([-112,234])) == np.array([0,1])).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70d00f24acf8d507a657e566bf406700",
     "grade": false,
     "grade_id": "cell-3d371c1a9510adba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**d) Implement the forward function and the cost function and its gradient (2.5P).**\n",
    "\n",
    "Implement the forward function for a neural network with one hidden layer. Keep in mind that output 2 and output 3 are only needed if you choose to do the Bonus Task.\n",
    "\n",
    "*Hint:* You only need to use the activation function in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d86a53a9c70b28329d587cacc9adbc26",
     "grade": true,
     "grade_id": "cell-ddd55abc575278ce",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward(X: npt.NDArray[np.float64], \n",
    "            Wh: npt.NDArray[np.float64], \n",
    "            Bh: npt.NDArray[np.float64], \n",
    "            Wo: npt.NDArray[np.float64], \n",
    "            Bo: npt.NDArray[np.float64]) -> Tuple[npt.NDArray[np.float64], \n",
    "                                                  npt.NDArray[np.float64], \n",
    "                                                  npt.NDArray[np.float64]]:\n",
    "    ''' \n",
    "    This is the forward function for making predictions. \n",
    "    In order to train the network later you also need to return the activation and pre-activation of the hidden layer.\n",
    "    \n",
    "    Input:\n",
    "        X: This is equivalent to the x in f(x). So to this value we want a prediction\n",
    "        Wh: weight matrix or vector for the hidden layer\n",
    "        Bh: bias for the hidden layer\n",
    "        Wo: weight matrix for the output layer\n",
    "        Bo: bias for the output layer\n",
    "\n",
    "    Output:\n",
    "        1. Prediction\n",
    "        2. Activation of the hidden layer\n",
    "        3. Pre-activation of the hidden layer\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aa9fafa1e8354a023c459bf7dc09941",
     "grade": true,
     "grade_id": "cell-61597567b468dbbf",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d54d6a82a17075365fac15097ea258a2",
     "grade": false,
     "grade_id": "cell-a798b1592661fd2f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The cost function (also known as *loss*) is given as follows: $$\\begin{align*} X &:= \\text{Input for forward function}\\\\ y &:= \\text{Correct label for } X \\\\ \\text{forward}(X) &:= \\text{Forward function that predicts a label for input } X\\\\ \\text{The cost function:}\\\\ \\text{cost}(X,y) &= \\frac{\\sum (\\text{forward}(X)-y)^2}{2} \\end{align*} $$\n",
    "\n",
    "This is a slight variation of the one you already learned in the lecture but will do just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea9b1da55b7705ba4ea025bdd215d359",
     "grade": true,
     "grade_id": "cell-9cd1af9bdd6992c7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost(X: npt.NDArray[np.float64], y: np.float64,\n",
    "            Wh: npt.NDArray[np.float64], \n",
    "            Bh: npt.NDArray[np.float64], \n",
    "            Wo: npt.NDArray[np.float64], \n",
    "            Bo: npt.NDArray[np.float64]) -> np.float64:\n",
    "    ''' \n",
    "    Quadratic loss function\n",
    "\n",
    "    Input:\n",
    "        X: Input for the value to predict\n",
    "        y: real value for the prediction\n",
    "        Wh: weight matrix or vector for the hidden layer\n",
    "        Bh: bias for the hidden layer\n",
    "        Wo: weight matrix for the output layer\n",
    "        Bo: bias for the output layer\n",
    "\n",
    "    Output:\n",
    "        cost\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a1e11864b96444d42f858d9f1871ab6",
     "grade": true,
     "grade_id": "cell-4253b8f8ec4d529a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97be3e2ce7438672d410b6d6f2645a6b",
     "grade": true,
     "grade_id": "cell-634c90ea8da4ecc0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cost_grad(output: np.float64, y: np.float64) -> np.float64:\n",
    "    ''' \n",
    "    Gradient of Quadratic loss function\n",
    "\n",
    "    Input:\n",
    "        output: predicted value\n",
    "        y: real value for the output\n",
    "\n",
    "    Output:\n",
    "        gradient of cost\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e196db3e85618f979799176fd2cf0e8",
     "grade": true,
     "grade_id": "cell-6fa874405335c61d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e11403fc820b7ef12ca4ccda4104760",
     "grade": false,
     "grade_id": "cell-f3509e30c92090a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Make yourself a clear image of what gradients are required and how they are calculated. We implemented the `backprop` function already for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dbba0ead107e3b50ba043c1cecfba54",
     "grade": false,
     "grade_id": "cell-467471a6e368a58a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backprop(X: npt.NDArray[np.float64],\n",
    "             output: npt.NDArray[np.float64],\n",
    "             y: npt.NDArray[np.float64],\n",
    "             Ah: npt.NDArray[np.float64],\n",
    "             Zh: npt.NDArray[np.float64],\n",
    "             Wo: npt.NDArray[np.float64]) -> Tuple[npt.NDArray[np.float64], \n",
    "                                                   npt.NDArray[np.float64], \n",
    "                                                   npt.NDArray[np.float64], \n",
    "                                                   npt.NDArray[np.float64]]:\n",
    "\n",
    "    '''\n",
    "    The backprop function should output the gradients to all the necessary vectors. \n",
    "     \n",
    "    Input:\n",
    "        X: This is equivalent to the x in f(x). So to this value we want a prediction\n",
    "        output: Predicted value\n",
    "        y: true value to X\n",
    "        Ah: activation of hidden layer\n",
    "        Zh: pre-activation of hidden layer\n",
    "        Wo: weights of output layer\n",
    "\n",
    "    Output:\n",
    "        1. Gradients of weights of hidden layer\n",
    "        2. Gradients of weights of output layer \n",
    "        3. Gradients of bias of hidden layer\n",
    "        4. Gradients of bias of output layer\n",
    "    '''\n",
    "    err_output: np.float64 = cost_grad(output, y) #last layer is linear, no gradient needed\n",
    "    err_hidden: npt.NDArray[np.float64] = np.dot(err_output, Wo.T) * grad_relu(Zh)\n",
    "\n",
    "    # Weight Gradients\n",
    "    dCdWh: npt.NDArray[np.float64] = np.dot(X, err_hidden)\n",
    "    dCdWo: npt.NDArray[np.float64] = np.dot(Ah.T, err_output)\n",
    "    \n",
    "    # Bias Gradients\n",
    "    dCdBh: npt.NDArray[np.float64] = np.sum(err_hidden, axis=0, keepdims=True)\n",
    "    dCdBo: npt.NDArray[np.float64] = np.sum(err_output, axis=0, keepdims=True)\n",
    "\n",
    "    return dCdWh, dCdWo, dCdBh, dCdBo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7d1afb2ffc9fec254a83ca2a684679b",
     "grade": false,
     "grade_id": "cell-e3afde8da9485e0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**e) Implement the training loop and train a simple function in the from of $f(x) = ax² + bx + c$ for example $f(x) = (x-5)^2$. But feel free to experiment and maybe try higher order functions too.**\n",
    "\n",
    "For that you have to implement *gradient descent* to update the weights and performs one epoch of the training cycle and after that the training loop. (3P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14ca9d663aca1a75b41ebc9f9bb59da4",
     "grade": true,
     "grade_id": "cell-edb04359f5df9ef7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x: List[float], \n",
    "                     y: List[float],\n",
    "                     lr: float, \n",
    "                     Wh: npt.NDArray[np.float64], \n",
    "                     Wo: npt.NDArray[np.float64], \n",
    "                     Bh: npt.NDArray[np.float64], \n",
    "                     Bo: npt.NDArray[np.float64]) -> Tuple[float, \n",
    "                                                           npt.NDArray[np.float64], \n",
    "                                                           npt.NDArray[np.float64], \n",
    "                                                           npt.NDArray[np.float64], \n",
    "                                                           npt.NDArray[np.float64]]:\n",
    "    ''' \n",
    "    Gradient descent gets a list of inputs and true outputs and iterates trough \n",
    "    them and performs a weight and bias update each step. \n",
    "\n",
    "    Input:\n",
    "        x: list of inputs in the same ordering as y\n",
    "        y: list of true outputs with the same ordering as x\n",
    "        lr: learning rate of gradient descent\n",
    "        Wh: weight matrix or vector for the hidden layer\n",
    "        Bh: bias for the hidden layer\n",
    "        Wo: weight matrix for the output layer\n",
    "        Bo: bias for the output layer\n",
    "    \n",
    "    Output:\n",
    "        1. average of cost function of input data (TODO: check formulation)\n",
    "        2. updated weights of hidden layer\n",
    "        3. updated weights of output layer\n",
    "        4. updated bias of hidden layer\n",
    "        5. updated bias of output layer\n",
    "    '''\n",
    "    costs: float = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "989a6ff256a20f76e98bf332eb164238",
     "grade": false,
     "grade_id": "cell-4bb6dc49743ff2b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Tests to check your implementation\n",
    "inputLayerSize_t = 1\n",
    "hiddenLayerSize_t = 5 # This is the value you can play around with\n",
    "outputLayerSize_t = 1 \n",
    "\n",
    "# Initialize Weights\n",
    "Wh_t = np.ones((inputLayerSize_t, hiddenLayerSize_t)) \n",
    "Wo_t = np.ones((hiddenLayerSize_t, outputLayerSize_t))\n",
    "\n",
    "# Initialize Biases\n",
    "# Small positive value to avoid Relu units dying too quickly\n",
    "Bh_t = np.full((1, hiddenLayerSize_t), 0.2)\n",
    "Bo_t = np.full((1, outputLayerSize_t), 0.2)\n",
    "\n",
    "# gradient_descent([1,2],[3,4], 0.001, Wh_t, Wo_t, Bh_t, Bo_t)\n",
    "\n",
    "# Forward inputs and outputs\n",
    "## Test 1\n",
    "x_1 = [1,2,3]\n",
    "y_1 = [3,2,1]\n",
    "Wh_1 = Wh_t\n",
    "Wo_1 = Wo_t\n",
    "Bh_1 = Bh_t\n",
    "Bo_1 = Bo_t\n",
    "\n",
    "tmp_1 = gradient_descent(x_1,y_1, 0.001, Wh_1, Wo_1, Bh_1, Bo_1)\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(np.round(tmp_1[i], 2))\n",
    "\n",
    "assert(np.round(tmp_1[0], 2) == np.round(42.57, 2))\n",
    "assert((np.round(tmp_1[1], 2) == np.round(np.array([[0.94, 0.94, 0.94, 0.94, 0.94]]), 2)).all())\n",
    "assert((np.round(tmp_1[2], 2) == np.round(np.array([[0.93], [0.93], [0.93], [0.93], [0.93]]), 2)).all())\n",
    "assert((np.round(tmp_1[3], 2) == np.round(np.array([[0.17, 0.17, 0.17, 0.17, 0.17]]), 2)).all())\n",
    "assert((np.round(tmp_1[4], 2) == np.round(np.array([[0.17]]), 2)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04dab28feabecdfa409f6e8d948862b3",
     "grade": true,
     "grade_id": "cell-4a7162dbc77a1cd2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3dce9f0ca72a2dc39e6fa9b1cb01e55",
     "grade": false,
     "grade_id": "cell-928872b54329966e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "The following is the initialization of the weights and biases. So you get consistent results. Feel free to experiment with the number of neurons in the hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67a6ce10a6c5b5b1a732ec01644faa37",
     "grade": false,
     "grade_id": "cell-44bf0ddbf23c622b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_network() -> Tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "    inputLayerSize: int = 1\n",
    "    hiddenLayerSize: int = 10 # This is the value you can play around with\n",
    "    outputLayerSize: int = 1 \n",
    "\n",
    "    # Initialize Weights\n",
    "    np.random.seed(0)\n",
    "    Wh: npt.NDArray = np.random.randn(inputLayerSize, hiddenLayerSize) /np.sqrt(inputLayerSize)\n",
    "    Wo: npt.NDArray = np.random.randn(hiddenLayerSize, outputLayerSize) /np.sqrt(hiddenLayerSize)\n",
    "\n",
    "    # Initialize Biases\n",
    "    # Small positive value to avoid Relu units dying too quickly\n",
    "    Bh: npt.NDArray = np.full((1, hiddenLayerSize), 0.2)\n",
    "    Bo: npt.NDArray = np.full((1, outputLayerSize), 0.2)\n",
    "\n",
    "    return Wh, Wo, Bh, Bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1eed77a6378f4dbdbae0c5afc4c10be",
     "grade": false,
     "grade_id": "cell-095bdf88390389d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Next is the generation of the training data. Here you can also change up the function which the network should learn or the interval in which the numbers are generated. \n",
    "\n",
    "*Hint:* Start with an easy function an later aim for something more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b53a5aef0f4e558cea17db18c93de8b0",
     "grade": false,
     "grade_id": "cell-ad6e0fed392b835d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f_x(x: float) -> float:\n",
    "    '''\n",
    "    Feel free to play around!\n",
    "    '''\n",
    "    # return 0.1 * ((x-5)**3 - 10 * x + 120) # Example of a more complex function\n",
    "    return (x-5)**2\n",
    "\n",
    "x = np.linspace(0, 10, 1001)\n",
    "y = np.array([ f_x(i)  for i in x])\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "738f59fa0020f05ed3a1244686ba2fea",
     "grade": false,
     "grade_id": "cell-e8c14444f4319468",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Now implement the training loop and train your network for 1000 epochs with a learning rate of 0.001. Shuffle the training data before each epoch.\n",
    "\n",
    "*Note:* The cost should go down if that is not the case check your code above. Are the derivatives correct?\n",
    "\n",
    "*Hint:* In our test runs the cost at the end for $f(x) = (x-5)^2$ was most of the time below 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aef08cfea05a0e9893bbcbc957dee17",
     "grade": true,
     "grade_id": "cell-5dc46853dbb96276",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Wh, Wo, Bh, Bo = init_network()\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fcf2fb03518a1d80062beae416aac71f",
     "grade": true,
     "grade_id": "cell-e277af25361b7e06",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81d3022d6b21648446e7c32773a056f3",
     "grade": false,
     "grade_id": "cell-b469b612703dd4a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Normally you also need test data in order to validate the accuracy of you network. But for now we only plot the predictions and the real values in the same plot so we can compare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a52ca2e0a416a2773ba2b358f5432ecf",
     "grade": false,
     "grade_id": "cell-4eb1b00d00a54ce6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_ax = np.linspace(0, 10, 1001)\n",
    "y = np.array([ f_x(i)  for i in x_ax])\n",
    "preds = np.array([forward(i, Wh, Bh, Wo, Bo)[0][0,0] for i in x_ax])\n",
    "plt.plot(x_ax, preds)\n",
    "plt.plot(x_ax,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d478f403d0ef7c9b7ce5a1dcff9bf75",
     "grade": false,
     "grade_id": "cell-b9af9c32500758d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task 2:  Occurrence and Intro to Spacy ~ 4P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15bf72b733a8ca935ca657569fdd5698",
     "grade": false,
     "grade_id": "cell-6c962c2f59cdb2af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "As discussed in class, <b>spaCy</b> is a useful open-source library that enables the user to perform several NLP tasks with high-quality results. It is not only helpful for beginners in NLP but also for advanced programmers who want to integrate NLP features into real products.\n",
    "\n",
    "For this exercise, you should only use spaCy; but you may use np and pandas if needed. Of course, you are also allowed to use the entire [Python Standard Library](https://docs.python.org/3.9/library/index.html). Please follow the instructions given below. In case of questions, use our Discussion forum in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e24541b3b487b7c2f23358db97c11b5",
     "grade": false,
     "grade_id": "cell-78fa5032bb4ce7e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import List, Mapping\n",
    "from spacy.tokens import Token\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "# download the language model if you haven't already (you may have to restart your Python kernel)\n",
    "# spacy.cli.download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# this is a bit of a hacky patch for spaCy's missing handling of contractions (haven't, she'll, I'm) in version 3\n",
    "# don't worry about it\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"n't\"}]], {\"LEMMA\": \"not\"})\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'ll\"}]], {\"LEMMA\": \"will\"})\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'ve\"}]], {\"LEMMA\": \"have\"})\n",
    "nlp.get_pipe(\"attribute_ruler\").add([[{\"LOWER\": \"'m\"}]], {\"LEMMA\": \"be\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "624e857152615035442b8bc7babf44f5",
     "grade": false,
     "grade_id": "cell-c68087e89ca81b4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### About the Corpus\n",
    "\n",
    "In this exercise, you will work with a real dataset of public english-language tweets for the keyword 'lockdown' posted between Dec 14th and Dec 22nd 2020. It was originally collected for use in a psychological experiment investigating the public perception of covid lockdowns.  \n",
    "\n",
    "Tweets were scraped from Twitter search results using the [snscrape](https://github.com/JustAnotherArchivist/snscrape) tool on Dec 22nd 2020. All links and @mentions were removed. The subset of the corpus you are working on has been further trimmed down to reduce spam and off-topic content present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ea230aa4f3bb8ff07ec9c06380ddd89",
     "grade": false,
     "grade_id": "cell-4ad49e2d8e06f864",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "To get started, you will have to read the dataset from the provided `tweets.txt` file. Each line in this file represents a single tweet. You will need to open and read the file before starting the other subtasks.\n",
    "\n",
    "**Hint 1**: Depending on how you read the dataset, you may have to remove linebreaks from the end of the tweets. You can use the [`rstrip`](https://docs.python.org/3.9/library/stdtypes.html) function to do so.  \n",
    "**Hint 2**: You may have to select 'utf-8' as the encoding when opening the file.  \n",
    "**Hint 3**: For this task you have to use some spaCy functions. You can find some useful information about spaCy tokens and their attributes [here](https://spacy.io/api/token). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c5fc6c5145d943649f81977c9849315",
     "grade": false,
     "grade_id": "cell-9d9cfe9b43f4a10d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**a) Tokenize each tweet in the dataset, then print the tokenized versions of the first five tweets (\"token1\", \"token2\", \"token3\"...). Use spaCy to solve this task. (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63dae344813f635191f6d9e23c6f6767",
     "grade": true,
     "grade_id": "cell-9feeadbd8acc05f3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results: List[str] = [] #put the tokenized tweets in this List\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "624af16853ae305daad6cf40740a880d",
     "grade": true,
     "grade_id": "cell-68637a973047e05a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab11ae7f8afe6ad9a36f98771ca863c9",
     "grade": false,
     "grade_id": "cell-699d3d5373579a41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**b) Implement the function `occurence_lowercase`. It shall calculate the (absolute) number of occurrences of each token that is in lowercase. Apply the function to our dataset of tweets and print the result (i.e. 'token: occurence') in descending order. Use spaCy to identify lowercased tokens. (1.5P)**\n",
    "\n",
    "**Hint**: Do not lowercase all tokens, instead identify and count all already lowercased tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "907a319cfede8153e27bf590247a64e0",
     "grade": true,
     "grade_id": "cell-3e4545931fba5f5f",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def occurence_lowercase(data: List[List[Token]]) -> Mapping[str, int]:\n",
    "    \"\"\"\n",
    "    Counts occurences of all lowercased tokens.\n",
    "    The type hints are suggestions only. Feel free to use whatever works for you.\n",
    "    \n",
    "    @param data: array-like object containing tokenized tweets from subtask a)\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc017167d1a1ad74b0b606fc275610e8",
     "grade": true,
     "grade_id": "cell-faf39cd29d83bb00",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e337965d45015943acb6d41959922e5",
     "grade": true,
     "grade_id": "cell-fed0af2f970bde5b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5cab490f433660616bb14937b46339e",
     "grade": true,
     "grade_id": "cell-9ad5b2c27f6e85a7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Print the result\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "982fdec1399e79055317abca704c443d",
     "grade": false,
     "grade_id": "cell-42638dc94106a304",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**c) Implement the function `occurence_no_punctuation`. It shall extract all tokens which occur exactly `five` times, excluding punctuation. Additionally it shall return the absolute occurence of these tokens similar to b). Apply the function to our dataset of tweets and print the result in descending order. Use spaCy to identify which tokens are considered to be punctuation. (1.5P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed63e5b2221a07220265a7cbba1b249b",
     "grade": true,
     "grade_id": "cell-ddefb636f5211ef8",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def occurence_no_punctuation(data: List[List[Token]]) -> Mapping[str, int]:\n",
    "    \"\"\"\n",
    "    Counts occurences of all tokens excluding punctuation and returns all with occurences greater or equal 5.\n",
    "    The type hints are suggestions only. Feel free to use whatever works for you.\n",
    "    \n",
    "    @param data: array-like object containing tokenized tweets from subtask a)\n",
    "    @return: array-like object with tokens and their counts\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa2832d28d626191bb230019f488ee49",
     "grade": true,
     "grade_id": "cell-2ea11982d76da352",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd50cdaa3290430194c9d42b3f4a6a4f",
     "grade": true,
     "grade_id": "cell-b8d52d2011e86edb",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dfbbccd44a179fc1e1ad2707efaee0c",
     "grade": true,
     "grade_id": "cell-297aab8aac8d8237",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Print the result\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8a29d1cde719e1d3f207e3a99aa0ba5",
     "grade": false,
     "grade_id": "cell-91e6981a9c77f477",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Matcher / Pattern finder ~ 6P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f92a97aa4fca06d651d81a4e9ae3bed6",
     "grade": false,
     "grade_id": "cell-df2c6bd60b3eb30b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a) Use the spaCy matcher to extract and print [proper nouns](https://en.wikipedia.org/wiki/Proper_and_common_nouns) that are longer than one token. Print each tweet from the dataset that contains at least one such proper noun together with all the matching proper nouns it contains. (2.5P)**\n",
    "\n",
    "The output for a given tweet should look something like this:  \n",
    "```\n",
    "I live in New York City and I like Hot Dogs & Coke\n",
    "    - New York City\n",
    "    - Hot Dogs\n",
    "```\n",
    "\n",
    "**Hint 1**: If there is a proper noun like 'New York City' you should only print 'New York City' and not 'New York', 'New York City', and 'York City'. As in the previous task, you can quickly test different patterns using [explosion.ai/demos/matcher](https://explosion.ai/demos/matcher).\n",
    "\n",
    "**Hint 2**: For this task you may have to use some functions that are not provided by spaCy.\n",
    "\n",
    "**Hint 3**: You can check out [explosion.ai/demos/matcher](https://explosion.ai/demos/matcher) to play around with different patterns. You can also refer to the [spaCy documentation of the Token class](https://spacy.io/api/token) for interesting attributes and the [spaCy matching documentation](https://spacy.io/usage/rule-based-matching/) for info on how to create patterns.\n",
    "\n",
    "*Sidenote: With the spaCy matcher you can do a lot more for example:*\n",
    "\n",
    "**Example 1**: All tokens that describe a date or time. 'Let's meet this evening.' ==> *this evening*  \n",
    "**Example 2**: Tokens describing appearance (e.g. adjectives after 'look'): 'It looks good.' ==> *looks good*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "165f660d9de1edb63c2c65b60ac3e7bb",
     "grade": true,
     "grade_id": "cell-18c996c8d1dfbe14",
     "locked": false,
     "points": 1.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "220bed5e32e836e24f9b6e9120797497",
     "grade": true,
     "grade_id": "cell-b4f3c682e20ba734",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: print the result here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fd642900b5cd1f7d5ddf92169bbfc7c",
     "grade": false,
     "grade_id": "cell-c1d97a90f7033801",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**b) What methods do you know in order to get the common base form. What are advantages and disadvantages? (1P)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ec130e07bebd68daede9c46fef7cb0c",
     "grade": true,
     "grade_id": "cell-38f9efc75e3115d9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3b5197059dd622c27707bfa689ab1197",
     "grade": false,
     "grade_id": "cell-45dc08dd37e22457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**c) Go over the entire dataset and find verbs and nouns (including proper nouns!) that share the same lemma. Print each lemma that is shared between at least one verb and one noun together with all distinct, lowercased, non-lemmatized nouns and verbs from the dataset that share that lemma. Every lemma should only be printed once. (2.5P)**\n",
    "\n",
    "The output for the lemma 'walk' may look like this...\n",
    "```\n",
    "lemma: walk; nouns: walk; verbs: walk, walking, walked;\n",
    "```\n",
    "... assuming the dataset contains a sentence like 'We walked (V) the walk (N) and still walk (V) it today. Walking (V) brings us great joy.'\n",
    "\n",
    "**Hint**: For this task you may need some functions which are not provided by spaCy. For example, you can join two dataframes and group strings by concatenating them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcc21d8e0f0fbf618295f4936739ee30",
     "grade": true,
     "grade_id": "cell-fcee09536e743c60",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemma_dict = defaultdict(lambda: (set(), set()))\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17a02227db697448593506d9bdd8039d",
     "grade": true,
     "grade_id": "cell-1bad7dd0777fd608",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ac01c6ce8e1b0f52bbdc01de9779449",
     "grade": true,
     "grade_id": "cell-802c027c08f4f1b3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: print the result here\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a20d425e20cf17e9279d88ad5222014",
     "grade": false,
     "grade_id": "cell-684915d3de4d1aab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**Submission:**\n",
    "\n",
    "Please upload your submission to Moodle before the next exercise session <font color=\"red\">(Nov 30, 23:59pm)</font>!\n",
    "\n",
    "Submission format: `Group_XX_Exercise_XX.zip`\n",
    "\n",
    "Your submission should contain your filled out Jupyter notebook (naming schema: `Group_XX_Exercise_XX.ipynb`) and any auxiliar files that are necessary to run your code (e.g., the datasets provided by us).\n",
    "\n",
    "Each submission must be handed in only once per group."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "63e82528548ff98f8043e71fd656867e40ac409f65579a94ba0e5517ed8ef972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
